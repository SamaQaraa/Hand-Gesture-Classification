# Hand Gesture Classification Using MediaPipe Landmarks from the HaGRID Dataset  

## ğŸ“Œ Overview  
This project focuses on classifying hand gestures using **landmark data extracted with MediaPipe** from the HaGRID dataset.  

The input is a **CSV file** containing hand landmarks (**x, y, z** coordinates of 21 key points).  
The output is a **trained machine learning model** that classifies hand gestures into different categories.  

---

## ğŸ–ï¸ Hand Landmarks  
The landmarks used in this project are based on **MediaPipe's hand tracking model**, which detects **21 key points** on a hand.  

![Hand Landmarks](img.png)  

Each keypoint has **(x, y, z) coordinates**, which are used to classify gestures.  

---

## ğŸ“‚ Dataset Details  
- The **HaGRID dataset** contains **18 different hand gestures**.  
- Each gesture is represented by **21 hand landmarks** extracted using MediaPipe.  
- The dataset is stored in a **CSV file**, where each row contains **landmark coordinates** and a **gesture label**.  

---

## ğŸš€ How to Run the Project  

### 1ï¸âƒ£ Clone the Repository  
```bash
git clone https://github.com/yourusername/hand-gesture-classification.git
cd hand-gesture-classification
```

### 2ï¸âƒ£ Install Dependencies  
```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Run the Model in Google Colab  
- Open **Project_ML.ipynb** in Google Colab.  
- Upload the dataset (`hand_landmarks_data.csv`).  
- Run the notebook step by step to train the model.  

### 4ï¸âƒ£ Real-Time Gesture Detection  
To test the model in real-time using your webcam, run:  
```bash
python real_time_demo.py
```

---

## ğŸ¥ Demo  

Here is a **real-time demo of the model running on my hand**:  

<video width="640" height="360" controls>  
  <source src="Demo.mp4" type="video/mp4">  
  Your browser does not support the video tag.  
</video>  

---

## ğŸ“Š Project Deliverables  

### âœ… Google Colab Notebook  
- **Data Loading** â€“ Importing the dataset.  
- **Data Visualization** â€“ Plotting hand landmarks.  
- **Preprocessing** â€“ Cleaning and normalizing data.  
- **Model Training** â€“ Testing different classifiers.  
- **Evaluation** â€“ Reporting accuracy, precision, recall, and F1-score.  
- **Conclusion** â€“ Selecting the best model.  

### âœ… Output Video  
ğŸ¥ The **Demo.mp4** file contains a short video demonstrating real-time classification.  

---

## ğŸ”¥ Notes & Improvements  
- Hand landmarks are **recentered** (wrist as origin) to handle scale differences.  
- The **z** coordinate does not require additional processing.  
- The **output is stabilized** by taking the **mode of predictions** over a window.  

---

## ğŸ“¢ Final Step  
Before submitting:  
âœ… Ensure your **GitHub repo** is public.  
âœ… Ensure the **Demo.mp4** video is inside your project folder.  
âœ… Run the code to verify everything works smoothly.  

ğŸš€ **Happy coding!**  
```

---

## ğŸ”¹ Important Notes:  
- **GitHub does NOT support direct video playback**. If you push `Demo.mp4` to GitHub, it will show as a downloadable file.  
- If you want **GitHub to display the video**, you need to **upload it to YouTube or Google Drive** and replace this line:  

  ```markdown
  <video width="640" height="360" controls>  
    <source src="Demo.mp4" type="video/mp4">  
    Your browser does not support the video tag.  
  </video>
  ```
  **With a link to your video**:  
  ```markdown
  [ğŸ¥ Watch the demo video here](https://your-video-link.com)
  ```

---

This version **includes your video and image**, making your project more complete! ğŸ‰ Let me know if you need any help! ğŸš€ğŸ”¥